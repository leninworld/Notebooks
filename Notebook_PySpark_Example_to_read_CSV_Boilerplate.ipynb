{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook PySpark Example to read CSV #Boilerplate",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Concepts you will learn in this notebook:\n",
        "- Use *Google Scholar* data set in CSV file format with fields research_interest, author_name, email.\n",
        "- Read *research_interest* field from a csv file in *Google Drive* into RDD and then use *flatMap* and *reduceByKey* to count occurance for each of the research_interest with a map function. *flatMap* helps to apply a transformation on a RDD/Dataframe and convert into another RDD/Dataframe. *reduceByKey* helps to merge the values of keys (words) by applying an reducing operator (add) on it. In our example, we apply add reducing operator on word occurance count on each document/row of a dataframe.\n",
        "- Using Aggregate function (mean) and sort by column of a dataframe.\n",
        "- Create a new column and fill it with a *User Defined Function (UDF)* applied on a field in spark dataframe.\n",
        "- Convert a RDD into a Dataframe with or without schema.\n",
        "- Apply filter on Spark dataframe.\n",
        "- Write spark dataframe as a single CSV to a Google Drive Folder.\n",
        "- Spark *Join* concept applied on dataframes."
      ],
      "metadata": {
        "id": "UOkVs5l3van2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### START of PRE-REQUISITE"
      ],
      "metadata": {
        "id": "v7W_A3C752oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Use apt-get to install basic libraries needed to enable pyspark"
      ],
      "metadata": {
        "id": "xJoH0z_g92BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpdsvwzOinoa",
        "outputId": "1c5ced67-d679-485e-9f8c-1ed57dbea5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [867 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,498 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [725 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [758 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,935 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Fetched 14.5 MB in 4s (3,871 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh-OqOgJigt5"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_distributed = False\n",
        "!pip install -q findspark\n",
        "!pip install pytorch_lightning\n",
        "\n",
        "# !pip install elephas\n",
        "# !pip install analytics-zoo\n",
        "# !pip install bigdl"
      ],
      "metadata": {
        "id": "iYqXcHiUjZyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db956a85-1733-4052-b06f-6e4cbf1f15a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.8-py3-none-any.whl (526 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 317 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 337 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 348 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 358 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 368 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 378 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 389 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 399 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 409 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 419 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 430 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 440 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 450 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 460 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 471 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 481 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 491 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 501 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 512 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 522 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 526 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 63.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 62.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ba8cb228331ebd117eb0160b59e1766eb75839f5b25b48a759f17df03a23bb26\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2022.1.0 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.8 torchmetrics-0.6.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "d6buCAqJjshJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "kjZEmPP5jwXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "FpRSRQoAj2fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mount Google Drive"
      ],
      "metadata": {
        "id": "f8-pVufG998C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive to the linux running this Colab application\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# list few directories (check access)\n",
        "! ls -ltr /content/drive/MyDrive/dataset/\n",
        "! ls -ltr /content/drive/MyDrive/data_processing/\n",
        "!ls -ltr /content/drive/MyDrive/data_processing/results/top_10_corona_dist/\n",
        "!ls -ltr /content/drive/MyDrive/ |head -2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsqhLY4pmfZy",
        "outputId": "4be31230-0224-4e16-d82f-5631d11a387f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 17\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_json\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_html\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_csv\n",
            "drwx------ 2 root root 4096 Jan  4 21:59  dataset_xml\n",
            "-rw------- 1 root root  151 Jan 11 02:09 'readME -> dataset.gdoc'\n",
            "total 8\n",
            "drwx------ 2 root root 4096 Jan  4 21:55 results\n",
            "drwx------ 2 root root 4096 Jan  4 21:55 intermediate\n",
            "total 1\n",
            "-rw------- 1 root root 255 Jan 13 19:16 part-00000-de3a3f7b-bff7-49f8-a6ff-8f4cfa8e6fad-c000.csv\n",
            "-rw------- 1 root root   0 Jan 13 19:16 _SUCCESS\n",
            "total 3854358\n",
            "-rw------- 1 root root   6116583 May 20  2011 Addresses.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variable Declarations"
      ],
      "metadata": {
        "id": "ICsaQf5y-BzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data sets\n",
        "path_covid = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-covid/cdc-pfizer-covid-19-vaccine-distribution-by-state.csv\"\n",
        "path_power = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-powerlifting-csv/openpowerlifting-2021-12-10-b420db66.csv\"\n",
        "path_titanic_train = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-titanic/train.txt\"\n",
        "path_google_scholar = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-google-scholar/output.csv\"\n",
        "# base output path\n",
        "path_out_base_result = \"/content/drive/MyDrive/data_processing/results/\""
      ],
      "metadata": {
        "id": "RYna-pF3lJpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output file for top 10 corona distribution\n",
        "path_out_avg = path_out_base_result + \"top_10_corona_dist\""
      ],
      "metadata": {
        "id": "QzRvASj1_a5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT important Libraries\n",
        "import pyspark.sql.functions as F\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import traceback\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from requests import get\n",
        "import requests\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import torch\n",
        "import matplotlib\n",
        "import pytorch_lightning\n",
        "from __future__ import print_function\n",
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "\n",
        "def pretty_print_pandas(title, df, n):\n",
        "  \"\"\" Pretty print\n",
        "  \"\"\"\n",
        "  print(f\"{title}:\")\n",
        "  print(tabulate(df.head(n), headers=\"keys\", tablefmt=\"psql\" ))"
      ],
      "metadata": {
        "id": "OHNdgXFmN-3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sample read few data sets available"
      ],
      "metadata": {
        "id": "KnHqLJ3p-I4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr /content/drive/MyDrive/dataset/dataset_csv/dataset-powerlifting-csv/openpowerlifting-2021-12-10-b420db66.csv\n",
        "!echo \"--------------> tit\"\n",
        "!ls -ltr /content/drive/MyDrive/dataset/dataset_csv/\n",
        "!echo \"--------------> tit_train\"\n",
        "!head -2 /content/drive/MyDrive/dataset/dataset_csv/dataset-titanic/train.csv\n",
        "!echo \"--------------> myDrive\"\n",
        "!ls -ltr /content/drive/MyDrive/ | head -5\n",
        "!head -2 /content/drive/MyDrive/dataset/dataset_csv/dataset-google-scholar/output.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4d9hFktmpjP",
        "outputId": "b7ec1c8c-678d-43f6-97bf-f07fc7b355b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 502549264 Dec 10 06:09 /content/drive/MyDrive/dataset/dataset_csv/dataset-powerlifting-csv/openpowerlifting-2021-12-10-b420db66.csv\n",
            "--------------> tit\n",
            "total 20\n",
            "drwx------ 2 root root 4096 Dec 10 06:56 dataset-powerlifting-csv\n",
            "drwx------ 2 root root 4096 Dec 11 07:34 dataset-covid\n",
            "drwx------ 2 root root 4096 Jan  3 19:48 dataset-titanic\n",
            "drwx------ 2 root root 4096 Jan  4 06:09 dataset-google-scholar\n",
            "drwx------ 2 root root 4096 Jan 11 02:13 dataset-covid-2\n",
            "--------------> tit_train\n",
            "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
            "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
            "--------------> myDrive\n",
            "total 3854358\n",
            "-rw------- 1 root root   6116583 May 20  2011 Addresses.mp3\n",
            "-rw------- 1 root root   5384507 May 20  2011 All in the Family.mp3\n",
            "-rw------- 1 root root   4926026 May 20  2011 Allergies.mp3\n",
            "-rw------- 1 root root   7512985 May 20  2011 Asking for Favors.mp3\n",
            "author_name,email,affiliation,coauthors_names,research_interest\n",
            "William Eberle,tntech.edu,Tennessee Technological University,,data_mining##anomaly_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ input files"
      ],
      "metadata": {
        "id": "Nr8ATpnUM1E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read covid vaccine weekly distribution \n",
        "df_in_covid = spark \\\n",
        "              .read \\\n",
        "              .option(\"header\", True) \\\n",
        "              .csv(path_covid)\n",
        "\n",
        "# read google scholar\n",
        "df_gs = spark \\\n",
        "        .read \\\n",
        "        .option(\"header\", True) \\\n",
        "        .csv(path_google_scholar)\n"
      ],
      "metadata": {
        "id": "xe1D7rboMvml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### END OF PRE-PREQUISITE"
      ],
      "metadata": {
        "id": "xPZHftCT6Gz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Scholar -> calculate frequency of research interest"
      ],
      "metadata": {
        "id": "nyOYfwW7xCLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE FREQUENCY FOR EACH WORD IN RESEARCH INTEREST\n",
        "%%time\n",
        "from pyspark import SparkContext\n",
        "from operator import add\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "# read into dataframe (df)\n",
        "df_gs = spark.read.option(\"header\", True).csv(path_google_scholar)\n",
        "# research_interest can't be None\n",
        "df_gs_clean = df_gs.filter(\"research_interest != 'None'\")\n",
        "# referring Column Names\n",
        "rdd_ri = df_gs_clean.rdd.map(lambda x: (x[\"research_interest\"]))\n",
        "print(\"\\nSample RDD rows:\")\n",
        "print(rdd_ri.take(5))\n",
        "print(\"\\nSample RDD rows after frequenc count for each words:\")\n",
        "# flatMap() helps to apply transformation\n",
        "rdd_ri_freq = rdd_ri.flatMap(lambda x: [(w.lower(), 1) for w in x.split('##')]).reduceByKey(add)\n",
        "# rdd print with take() function\n",
        "print(rdd_ri_freq.take(5))\n",
        "\n",
        "# approach 1 : convert to df without any schema (no proper col names)\n",
        "df_ri_freq = rdd_ri_freq.toDF() \n",
        "\n",
        "pretty_print_pandas(\"RI freq without schema\", df_ri_freq, 10)\n",
        "\n",
        "# approach 2 : convert to df with schema\n",
        "schema = StructType([StructField(\"ri\", StringType(), False), \n",
        "                     StructField(\"frequency\", IntegerType(), False)\n",
        "])\n",
        "# convert rdd to df with schema\n",
        "df = spark.createDataFrame(rdd_ri_freq, schema)\n",
        "print(\"\\nProposed Schema of DF:\")\n",
        "# print schema (to verify)\n",
        "df.printSchema()\n",
        "print(\"\\nRDD converted to DF with schema:\")\n",
        "# sort\n",
        "df_sort = df.sort(F.col(\"frequency\").desc())\n",
        "df_sort.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asgW9dk08_Ux",
        "outputId": "31e90d77-322d-49cd-c6b3-1df05db61515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample RDD rows:\n",
            "['data_mining##anomaly_detection', 'artificial_intelligence##machine_learning##data_mining##graph_mining##security', 'machine_learning##never_ending_learning##lifelong_machine_learning##medical_informatics', 'graph_mining##big_data_analytics##machine_learning', 'network_security##cyber_physical_systems_security##cyber_education_and_workforce_development']\n",
            "\n",
            "Sample RDD rows after frequenc count for each words:\n",
            "[('data_mining', 63), ('anomaly_detection', 5), ('artificial_intelligence', 123), ('machine_learning', 198), ('graph_mining', 5)]\n",
            "RI freq without schema:\n",
            "+---------------------------+-----+\n",
            "| 0                         |   1 |\n",
            "|---------------------------+-----|\n",
            "| data_mining               |  63 |\n",
            "| anomaly_detection         |   5 |\n",
            "| artificial_intelligence   | 123 |\n",
            "| machine_learning          | 198 |\n",
            "| graph_mining              |   5 |\n",
            "| security                  |  25 |\n",
            "| never_ending_learning     |   1 |\n",
            "| lifelong_machine_learning |   1 |\n",
            "| medical_informatics       |   7 |\n",
            "| big_data_analytics        |   5 |\n",
            "+---------------------------+-----+\n",
            "\n",
            "Proposed Schema of DF:\n",
            "root\n",
            " |-- ri: string (nullable = false)\n",
            " |-- frequency: integer (nullable = false)\n",
            "\n",
            "\n",
            "RDD converted to DF with schema:\n",
            "+---------------------------+---------+\n",
            "|ri                         |frequency|\n",
            "+---------------------------+---------+\n",
            "|machine_learning           |198      |\n",
            "|artificial_intelligence    |123      |\n",
            "|software_engineering       |94       |\n",
            "|computer_vision            |81       |\n",
            "|data_mining                |63       |\n",
            "|natural_language_processing|63       |\n",
            "|robotics                   |53       |\n",
            "|human_computer_interaction |52       |\n",
            "|information_retrieval      |42       |\n",
            "|deep_learning              |36       |\n",
            "+---------------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "CPU times: user 97.2 ms, sys: 10.8 ms, total: 108 ms\n",
            "Wall time: 1.19 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This example takes all the columns in the given google scholar file and process the rdd\n",
        "\n",
        "# rdd\n",
        "rdd = spark.sparkContext.textFile(path_google_scholar)\n",
        "print(type(rdd))\n",
        "counts = rdd.flatMap(lambda x: [(w.lower(), 1) for w in x.split(',')]).reduceByKey(add)\n",
        "print(counts.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSq-H4G4c2Sv",
        "outputId": "57916c42-e0b4-407e-ae29-d3d01183fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "[('author_name', 1), ('email', 1), ('affiliation', 1), ('coauthors_names', 1), ('research_interest', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UDF to create a new field \"is_artificial_intellence\" of boolean type"
      ],
      "metadata": {
        "id": "rNul9-kNgIhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType\n",
        "import traceback\n",
        "\n",
        "lst_ai  = [\"data_science\", \"artificial_intelligence\",\n",
        "           \"machine_learning\"]\n",
        "\n",
        "@F.udf\n",
        "def is_ai(research):\n",
        "    \"\"\" return 1 if research in AI domain else 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # split the research interest string with delimiter \"##\"  \n",
        "      lst_research = [w.lower() for w in str(research).split(\"##\")]\n",
        "\n",
        "      for res in lst_research:\n",
        "        # if present in AI domain\n",
        "        if res in lst_ai:\n",
        "          return 1\n",
        "      # not present in AI domain\n",
        "      return 0\n",
        "    except:\n",
        "      return -1\n",
        " \n",
        "# df read \n",
        "df_gs = spark.read.option(\"header\", True).csv(path_google_scholar)\n",
        "# create a new column \"is_artificial_intelligence\"\n",
        "df_gs_new = df_gs.withColumn(\"is_artificial_intelligence\",\\\n",
        "                             is_ai(F.col(\"research_interest\")))\n",
        "# df_gs_new.printSchema()\n",
        "df_gs.show(5, truncate=False)\n",
        "df_gs_new.show(n=20)\n",
        "print(f\"Verify that is_ai should have only two distinct value: 0 & 1\")\n",
        "df_gs_new.select(\"is_ai\").distinct().show(5)\n",
        "# show selective columns for analysis\n",
        "df_gs_new[df_gs_new[\"author_name\"].isin([\"Christa Cody\", \"Gabriel Weimann\", \"\"])]\\\n",
        "    .select(\"author_name\",\"research_interest\",\"is_artificial_intelligence\")\\\n",
        "    .show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OE25zYCgHJV",
        "outputId": "72a07ba4-ac9e-4187-c8aa-b61d8162ea9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|author_name           |email             |affiliation                       |coauthors_names                                                                                                                                      |research_interest                                                                           |\n",
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|William Eberle        |tntech.edu        |Tennessee Technological University|null                                                                                                                                                 |data_mining##anomaly_detection                                                              |\n",
            "|Lawrence Holder       |wsu.edu           |Washington State University       |Diane J Cook##William Eberle                                                                                                                         |artificial_intelligence##machine_learning##data_mining##graph_mining##security              |\n",
            "|Talbert DA            |tntech.edu        |Tennessee Technological University|null                                                                                                                                                 |machine_learning##never_ending_learning##lifelong_machine_learning##medical_informatics     |\n",
            "|Dr. Sirisha Velampalli|crraoaimscs.res.in|null                              |William Eberle##Lenin Mookiah                                                                                                                        |graph_mining##big_data_analytics##machine_learning                                          |\n",
            "|Ambareen Siraj        |tntech.edu        |Tennessee Tech University         |Rayford Vaughn##William Eberle##Siddharth Kaza##Christa Cody##Khaled Rabieh##Mohammad Ashiqur Rahman##Lenin Mookiah##Ferrol Aderholdt##Brandon Malone|network_security##cyber_physical_systems_security##cyber_education_and_workforce_development|\n",
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
            "|         author_name|              email|         affiliation|     coauthors_names|   research_interest|is_ai|\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
            "|      William Eberle|         tntech.edu|Tennessee Technol...|                null|data_mining##anom...|    0|\n",
            "|     Lawrence Holder|            wsu.edu|Washington State ...|Diane J Cook##Wil...|artificial_intell...|    1|\n",
            "|          Talbert DA|         tntech.edu|Tennessee Technol...|                null|machine_learning#...|    1|\n",
            "|Dr. Sirisha Velam...| crraoaimscs.res.in|                null|William Eberle##L...|graph_mining##big...|    1|\n",
            "|      Ambareen Siraj|         tntech.edu|Tennessee Tech Un...|Rayford Vaughn##W...|network_security#...|    0|\n",
            "|        Christa Cody|           ncsu.edu|North Carolina St...|                null|artificial_intell...|    1|\n",
            "|      Siddharth Kaza|         towson.edu|   Towson University|Byron Marshall##J...|                null|    0|\n",
            "|    Ferrol Aderholdt|         nvidia.com|              NVIDIA|                null|                null|    0|\n",
            "|       Khaled Rabieh|           shsu.edu|                null|Mohamed M. E. A. ...|information_secur...|    0|\n",
            "|      Byron Marshall|bus.oregonstate.edu|Oregon State Univ...|Hsinchun Chen##Si...|it_security##know...|    0|\n",
            "|       Peter Kawalek|        lboro.ac.uk|Loughborough Univ...|                null|information_syste...|    0|\n",
            "|Fatemeh “Mariam” ...|            uwm.edu|University of Wis...|Gaurav Bansal##Ja...|web_based_it_syst...|    0|\n",
            "|Mohamed M. E. A. ...|         tntech.edu|Tennessee Tech Un...|                null|security_and_priv...|    0|\n",
            "|      Josh Dehlinger|         towson.edu|   Towson University|Robyn Lutz##Suran...|software_engineering|    0|\n",
            "|          Jaeki Song|            ttu.edu|Texas Tech Univer...|                null|                null|    0|\n",
            "|       Wingyan Chung|            wcu.edu|Western Carolina ...|Hsinchun Chen##Da...|business_analytic...|    1|\n",
            "|        Ahmed Abbasi|             nd.edu|University of Not...|Hsinchun Chen##Fa...|artificial_intell...|    1|\n",
            "|     Gabriel Weimann|    com.haifa.ac.il| University of Haifa|Jonathan Cohen##W...|political_communi...|    0|\n",
            "|       Lenin Mookiah|students.tntech.edu|                null|William Eberle##L...|graph_mining##ano...|    0|\n",
            "|        Diane J Cook|       eecs.wsu.edu|Washington State ...|Lawrence Holder##...|artificial_intell...|    1|\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "Verify that is_ai should have only two distinct value: 0 & 1\n",
            "+-----+\n",
            "|is_ai|\n",
            "+-----+\n",
            "|    0|\n",
            "|    1|\n",
            "+-----+\n",
            "\n",
            "+---------------+-----------------------------------------------------------------+-----+\n",
            "|author_name    |research_interest                                                |is_ai|\n",
            "+---------------+-----------------------------------------------------------------+-----+\n",
            "|Christa Cody   |artificial_intelligence##machine_learning##educational_technology|1    |\n",
            "|Gabriel Weimann|political_communication##terrorism##media_effects                |0    |\n",
            "+---------------+-----------------------------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD TO DATA FRAME (MINIMALIST EXAMPLE)\n",
        "rdd2 = df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2))  \n",
        "df2 = rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
        "df2.show()\n"
      ],
      "metadata": {
        "id": "_SfjsF-nnz-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference:\n",
        "[View RDD content](https://stackoverflow.com/questions/25295277/view-rdd-contents-in-python-spark)\n"
      ],
      "metadata": {
        "id": "XbFCm7Id9sCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TOP 10 VACCINE WEEKLY 1ST DOES DISTRIBUTION STATES"
      ],
      "metadata": {
        "id": "hwyicBr9siTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Write as CSV to Google Drive"
      ],
      "metadata": {
        "id": "v-3UnMAMPWcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print sample\n",
        "df_in_covid.show(n=2, truncate=False)\n",
        "# group by average\n",
        "df_avg_1 = df_in_covid.groupby(\"jurisdiction\")\\\n",
        "  .agg(F.avg(\"_1st_dose_allocations\")\n",
        "  .alias(\"avg\"))\\\n",
        "  .sort(F.col(\"avg\").desc())\\\n",
        "  .toDF(\"state\", \"avg\")\n",
        "\n",
        "print(\"Top 10 States by 1st dose covid vaccine distribution\")\n",
        "df_avg_1.show(n=10)\n",
        "print(type(df_avg_1))\n",
        "# write top 10 by average corona weekly vaccine states \n",
        "df_avg_1.limit(10) \\\n",
        "        .coalesce(1) \\\n",
        "        .write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"header\", True) \\\n",
        "        .option(\"quoteAll\",True) \\\n",
        "        .csv(path_out_avg)"
      ],
      "metadata": {
        "id": "rnZTzFOPf9vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dad945f-1c29-41dc-b846-6452adb3e218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------------------+---------------------+---------------------+\n",
            "|jurisdiction|week_of_allocations    |_1st_dose_allocations|_2nd_dose_allocations|\n",
            "+------------+-----------------------+---------------------+---------------------+\n",
            "|Connecticut |2021-06-21T00:00:00.000|54360                |54360                |\n",
            "|Maine       |2021-06-21T00:00:00.000|21420                |21420                |\n",
            "+------------+-----------------------+---------------------+---------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Top 10 States by 1st dose covid vaccine distribution\n",
            "+----------------+----------+\n",
            "|           state|       avg|\n",
            "+----------------+----------+\n",
            "|      California|  561307.5|\n",
            "|           Texas| 384333.75|\n",
            "|         Florida|306883.125|\n",
            "|Federal Entities|  213150.0|\n",
            "|            Ohio| 168761.25|\n",
            "|    Pennsylvania|166415.625|\n",
            "|        New York| 164036.25|\n",
            "|  North Carolina| 147026.25|\n",
            "|         Georgia| 146036.25|\n",
            "|        Illinois| 146036.25|\n",
            "+----------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Rename the Spark dataframe written non-human readable CSV file to human-readable one"
      ],
      "metadata": {
        "id": "yEsUc-QhPYow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "import os\n",
        "\n",
        "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
        "    \"\"\" return list of filenames that ends with suffix\n",
        "    \"\"\"\n",
        "    filenames = listdir(path_to_dir)\n",
        "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
        "\n",
        "# get file name that just wrote (csv file name)\n",
        "path_csv_file_path = path_out_avg + \"/\" + find_csv_filenames(path_out_avg)[0]\n",
        "# output\n",
        "path_new_file = path_out_avg + \"/\" + \"top_10_states.csv\"\n",
        "# old file name and new file name\n",
        "print(f\"path_csv_file_path: {path_csv_file_path} \\n path_new_file: {path_new_file}\")\n",
        "# rename file\n",
        "os.rename(path_csv_file_path, path_new_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0dxkJkoFkx3",
        "outputId": "3849bec4-c721-44fe-a96d-c2dce8e54c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path_csv_file_path: /content/drive/MyDrive/data_processing/results/top_10_corona_dist/top_10_states.csv \n",
            " path_new_file: /content/drive/MyDrive/data_processing/results/top_10_corona_dist/top_10_states.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### PANDAS: Calculate Average For Each State"
      ],
      "metadata": {
        "id": "bTyOqdUMO7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PANDAS - Example of reading a csv and writing as a CSV\n",
        "path_new_file = path_out_avg + \"/\" + \"top_10_states.csv\"\n",
        "print(f\"input file -> {path_new_file}\")\n",
        "# read already existing file\n",
        "df_in_state = pd.read_csv(path_new_file)\n",
        "# sample\n",
        "print(df_in_state.head(2))\n",
        "# calculate avg for each state (the input file already have avg)\n",
        "# just example to use pandas to do same operation of average\n",
        "df_in_top10 = df_in_state.groupby(\"state\")[\"avg\"].mean().to_frame(\"avg\").reset_index()\n",
        "# output 2\n",
        "path_new_file_pd = path_out_avg + \"/\" + \"top_10_states_pandas.csv\"\n",
        "print(f\"output to {path_new_file_pd}\")\n",
        "# write pandas df to csv\n",
        "df_in_top10.to_csv(path_new_file_pd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6knMVrNNpGG",
        "outputId": "2108fbc8-370c-4a15-823e-3934a724269f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input file -> /content/drive/MyDrive/data_processing/results/top_10_corona_dist/top_10_states.csv\n",
            "        state        avg\n",
            "0  California  561307.50\n",
            "1       Texas  384333.75\n",
            "output to /content/drive/MyDrive/data_processing/results/top_10_corona_dist/top_10_states_pandas.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPARK: Average of 1st and 2nd DOSE"
      ],
      "metadata": {
        "id": "G8HU4IUe4Vst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each state, calculate average # 1st dose, average # 2nd dose\n",
        "\n",
        "print(list(df_in_covid))\n",
        "\n",
        "# calculate average weekly 1st dose vaccine distribution\n",
        "df_avg_1 = df_in_covid.groupby(\"jurisdiction\")\\\n",
        "  .agg(F.avg(\"_1st_dose_allocations\").alias(\"avg_1\"), \\\n",
        "       F.avg(\"_2nd_dose_allocations\").alias(\"avg_2\"), \\\n",
        "       F.sum(\"_1st_dose_allocations\").alias(\"sum_1\"), \\\n",
        "       F.sum(\"_2nd_dose_allocations\").alias(\"sum_2\")\n",
        "       ) \\\n",
        "  .sort(F.col(\"avg_1\").desc())\n",
        "\n",
        "\n",
        "df_avg_1.show(15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqox4liGIXUE",
        "outputId": "653a562c-502b-40fb-8399-e7ab9c07f002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Column<'jurisdiction'>, Column<'week_of_allocations'>, Column<'_1st_dose_allocations'>, Column<'_2nd_dose_allocations'>]\n",
            "+----------------+----------+----------+---------+---------+\n",
            "|    jurisdiction|     avg_1|     avg_2|    sum_1|    sum_2|\n",
            "+----------------+----------+----------+---------+---------+\n",
            "|      California|  561307.5|  561307.5|8980920.0|8980920.0|\n",
            "|           Texas| 384333.75| 384333.75|6149340.0|6149340.0|\n",
            "|         Florida|306883.125|306883.125|4910130.0|4910130.0|\n",
            "|Federal Entities|  213150.0|  213150.0|3197250.0|3197250.0|\n",
            "|            Ohio| 168761.25| 168761.25|2700180.0|2700180.0|\n",
            "|    Pennsylvania|166415.625|166415.625|2662650.0|2662650.0|\n",
            "|        New York| 164036.25| 164036.25|2624580.0|2624580.0|\n",
            "|  North Carolina| 147026.25| 147026.25|2352420.0|2352420.0|\n",
            "|         Georgia| 146036.25| 146036.25|2336580.0|2336580.0|\n",
            "|        Illinois| 146036.25| 146036.25|2336580.0|2336580.0|\n",
            "|        Michigan|144826.875|144826.875|2317230.0|2317230.0|\n",
            "|      New Jersey| 129138.75| 129138.75|2066220.0|2066220.0|\n",
            "|   New York City| 125178.75| 125178.75|2002860.0|2002860.0|\n",
            "|        Virginia| 122906.25| 122906.25|1966500.0|1966500.0|\n",
            "|      Washington|  108126.0|  108126.0|1621890.0|1621890.0|\n",
            "+----------------+----------+----------+---------+---------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Power-lifting* data set => spark aggregate & filter example"
      ],
      "metadata": {
        "id": "H83QM7PLSy6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(path_power)\n",
        "# read power csv\n",
        "df_power = spark.read.option(\"header\", True).csv(path_power)\n",
        "print(df_power.show(2))\n",
        "# groupby Sex\n",
        "df2 = df_power.groupby(\"Sex\").agg(F.countDistinct(\"AgeClass\")).toDF(\"gender\", \"count\")\n",
        "df2.show(n=2)\n",
        "# Sex \"Mx\" -> Show list of \"AgeClass\" available\n",
        "print(\"Mx\")\n",
        "df_power.filter(\"Sex == 'Mx'\").select(\"AgeClass\").distinct().show(n=5)\n",
        "# Sex \"M\" -> Show list of \"AgeClass\" available\n",
        "df_power.filter(\"Sex == 'M'\").select(\"AgeClass\").distinct().show(n=5)\n",
        "# who did the Best3SquatKg\n",
        "df_best_3squat = df_power.groupby(\"Sex\").agg(F.max(\"Best3SquatKg\")).toDF(\"Sex\", \"Best3SquatKgMax\")\n",
        "df_best_3squat.show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQBTKGETrnUu",
        "outputId": "9837d0c5-57c0-4def-9f2d-c209f19380e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset_csv/dataset-powerlifting-csv/openpowerlifting-2021-12-10-b420db66.csv\n",
            "+------------------+---+-----+---------+---+--------+--------------+--------+------------+-------------+--------+--------+--------+--------+------------+--------+--------+--------+--------+------------+-----------+-----------+-----------+-----------+---------------+-------+-----+------+------+------------+--------+------+-------+-----+----------+----------------+----------+-----------+---------+--------+---------------+\n",
            "|              Name|Sex|Event|Equipment|Age|AgeClass|BirthYearClass|Division|BodyweightKg|WeightClassKg|Squat1Kg|Squat2Kg|Squat3Kg|Squat4Kg|Best3SquatKg|Bench1Kg|Bench2Kg|Bench3Kg|Bench4Kg|Best3BenchKg|Deadlift1Kg|Deadlift2Kg|Deadlift3Kg|Deadlift4Kg|Best3DeadliftKg|TotalKg|Place|  Dots| Wilks|Glossbrenner|Goodlift|Tested|Country|State|Federation|ParentFederation|      Date|MeetCountry|MeetState|MeetTown|       MeetName|\n",
            "+------------------+---+-----+---------+---+--------+--------------+--------+------------+-------------+--------+--------+--------+--------+------------+--------+--------+--------+--------+------------+-----------+-----------+-----------+-----------+---------------+-------+-----+------+------+------------+--------+------+-------+-----+----------+----------------+----------+-----------+---------+--------+---------------+\n",
            "|       Alona Vladi|  F|  SBD|      Raw| 33|   24-34|         24-39|       O|        58.3|           60|      75|      80|     -90|    null|          80|      50|      55|      60|    null|          60|         95|        105|      107.5|       null|          107.5|  247.5|    1|279.44|282.18|      249.42|   57.10|   Yes| Russia| null|       GFP|            null|2019-05-11|     Russia|     null| Bryansk|Open Tournament|\n",
            "|Galina Solovyanova|  F|  SBD|      Raw| 43|   40-44|         40-49|      M1|        73.1|           75|      95|     100|     105|    null|         105|    62.5|    67.5|   -72.5|    null|        67.5|        100|        110|       -120|       null|            110|  282.5|    1|278.95|272.99|      240.35|   56.76|   Yes| Russia| null|       GFP|            null|2019-05-11|     Russia|     null| Bryansk|Open Tournament|\n",
            "+------------------+---+-----+---------+---+--------+--------------+--------+------------+-------------+--------+--------+--------+--------+------------+--------+--------+--------+--------+------------+-----------+-----------+-----------+-----------+---------------+-------+-----+------+------+------------+--------+------+-------+-----+----------+----------------+----------+-----------+---------+--------+---------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     F|   16|\n",
            "|    Mx|    3|\n",
            "+------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "Mx\n",
            "+--------+\n",
            "|AgeClass|\n",
            "+--------+\n",
            "|    null|\n",
            "|   13-15|\n",
            "|   24-34|\n",
            "|   20-23|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|AgeClass|\n",
            "+--------+\n",
            "|   16-17|\n",
            "|   65-69|\n",
            "|   18-19|\n",
            "|  80-999|\n",
            "|   75-79|\n",
            "+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+---------------+\n",
            "|Sex|Best3SquatKgMax|\n",
            "+---+---------------+\n",
            "|  F|          99.88|\n",
            "| Mx|             95|\n",
            "|  M|           99.9|\n",
            "+---+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark INNER JOIN EXAMPLE"
      ],
      "metadata": {
        "id": "LxTdsrQgS9-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an alias for each of the dataframes to be joined\n",
        "A = df_power.alias(\"A\")\n",
        "B = df_best_3squat.alias(\"B\")\n",
        "# join on sex column\n",
        "df_join = A.join(B, F.col(\"A.Sex\") == F.col(\"B.Sex\"), 'inner')\n",
        "lst_interest = [\"A.Name\", \"A.Sex\", \"A.Best3SquatKg\", \"B.Best3SquatKgMax\"]\n",
        "df_join.filter(\"A.Sex == 'F'\").select(*lst_interest).show(n=3, truncate=False)\n",
        "df_join.filter(\"A.Sex == 'M'\").select(*lst_interest).show(n=3, truncate=False)\n",
        "df_join.filter(\"A.Sex == 'Mx'\").select(*lst_interest).show(n=3, truncate=False)\n",
        "# id and name occurs in both df\n",
        "# df_join = A.join(B, ['id', 'name'], 'inner')"
      ],
      "metadata": {
        "id": "7jot8bMjzi6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark INNER JOIN EXAMPLE"
      ],
      "metadata": {
        "id": "1qkESE0hIUIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm\n",
        "https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/\n",
        "https://sparkbyexamples.com/spark/print-the-contents-of-rdd-in-spark-pyspark/\n",
        "# pipelineds RDD creation when using map operation\n",
        "https://stackoverflow.com/questions/44355416/need-instance-of-rdd-but-returned-class-pyspark-rdd-pipelinedrdd"
      ],
      "metadata": {
        "id": "yFQ0JaGJf-p8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}